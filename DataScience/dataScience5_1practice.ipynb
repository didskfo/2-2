{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNkWtce5pxu_",
        "outputId": "04ad2957-06c2-4cfb-e086-977e03a2cb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4909,  0.4914, -0.6061],\n",
            "        [ 1.3866, -0.4013,  1.3471],\n",
            "        [-0.6585,  0.7748,  0.6029],\n",
            "        [-1.5096, -1.0390, -0.6979]], requires_grad=True)\n",
            "tensor([[-0.2425, -0.7292, -0.6995]], requires_grad=True)\n",
            "0 5.412178039550781\n",
            "100 0.3581276535987854\n",
            "200 0.23492926359176636\n",
            "300 0.16263993084430695\n",
            "400 0.11901606619358063\n",
            "500 0.09087298810482025\n",
            "600 0.07163464277982712\n",
            "700 0.05788714438676834\n",
            "800 0.04771832376718521\n",
            "900 0.03998439759016037\n",
            "1000 0.03396528214216232\n",
            "1100 0.029188405722379684\n",
            "1200 0.025333194062113762\n",
            "1300 0.022176366299390793\n",
            "1400 0.01955818384885788\n",
            "1500 0.017362207174301147\n",
            "1600 0.015501974150538445\n",
            "1700 0.013912028633058071\n",
            "1800 0.012542258948087692\n",
            "1900 0.011353641748428345\n",
            "2000 0.010315579362213612\n",
            "2100 0.009403535164892673\n",
            "2200 0.008598034270107746\n",
            "2300 0.007883092388510704\n",
            "2400 0.007245650514960289\n",
            "2500 0.006674974225461483\n",
            "2600 0.006162156350910664\n",
            "2700 0.005699690897017717\n",
            "2800 0.005281282588839531\n",
            "2900 0.004901614971458912\n",
            "3000 0.004556088242679834\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]])\n",
        "y_train = torch.FloatTensor([[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]])\n",
        "\n",
        "W = torch.randn(4,3,requires_grad=True)\n",
        "b = torch.randn(1,3,requires_grad=True)\n",
        "\n",
        "optim = torch.optim.Adam([W,b],lr=0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "\n",
        "  h = torch.softmax(torch.mm(x_train, W)+b, dim=1)\n",
        "  # h = (torch.mm(x_train, W)+b).softmax(dim=1)\n",
        "  cost = -torch.mean(torch.sum(y_train * torch.log(h), dim=1))\n",
        "  # cost = -(y_train*torch.log(h)).sum(dim=1).mean()\n",
        "\n",
        "  # h = torch.mm(x_train, W)+b\n",
        "  # cost = -torch.mean(torch.sum(y_train * torch.log(torch.softmax(h,dim=1)), dim=1))\n",
        "\n",
        "  optim.zero_grad()\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 100 == 0:\n",
        "      print(epoch, cost.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.tensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]], dtype=torch.float)\n",
        "\n",
        "h_test = torch.softmax(torch.mm(x_test,W)+b,dim=1)\n",
        "print(h_test)\n",
        "print(torch.argmax(h_test, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmnwNqIqpx_",
        "outputId": "faa20b4f-c0d9-4539-8e93-949e2a4a0883"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 1.7427e-17, 1.0706e-34],\n",
            "        [1.8421e-03, 6.7243e-01, 3.2572e-01],\n",
            "        [6.7848e-31, 1.1033e-10, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "x_train = torch.FloatTensor([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]])\n",
        "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
        "\n",
        "# W = torch.randn(4,3,requires_grad=True)\n",
        "# b = torch.randn(1,3,requires_grad=True)\n",
        "# optim = torch.optim.Adam([W,b],lr=0.1)\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "\n",
        "  # h = torch.mm(x_train, W)+b\n",
        "  h = model(x_train)\n",
        "  cost = F.cross_entropy(h, y_train)\n",
        "\n",
        "  optim.zero_grad()\n",
        "  cost.backward()\n",
        "  optim.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 100 == 0:\n",
        "      print(epoch, cost.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex6bZmAtvW-l",
        "outputId": "fcc7c336-f30f-44f3-c336-e8e96524152e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.257699966430664\n",
            "100 0.28990671038627625\n",
            "200 0.16692619025707245\n",
            "300 0.1083718091249466\n",
            "400 0.07638566941022873\n",
            "500 0.05684909597039223\n",
            "600 0.043996281921863556\n",
            "700 0.03507738560438156\n",
            "800 0.028629301115870476\n",
            "900 0.02381208911538124\n",
            "1000 0.020114833489060402\n",
            "1100 0.01721263863146305\n",
            "1200 0.01489066518843174\n",
            "1300 0.013002232648432255\n",
            "1400 0.011444682255387306\n",
            "1500 0.010144088417291641\n",
            "1600 0.009046223014593124\n",
            "1700 0.008110626600682735\n",
            "1800 0.007306527346372604\n",
            "1900 0.006610112264752388\n",
            "2000 0.006002909503877163\n",
            "2100 0.005470147356390953\n",
            "2200 0.0050001246854662895\n",
            "2300 0.004583302419632673\n",
            "2400 0.0042119198478758335\n",
            "2500 0.0038797049783170223\n",
            "2600 0.0035812901332974434\n",
            "2700 0.0033122936729341745\n",
            "2800 0.003069015685468912\n",
            "2900 0.002848322270438075\n",
            "3000 0.002647530287504196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = np.array([[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]])\n",
        "y_train = np.array([2,2,2,1,1,1,0,0])\n",
        "\n",
        "model = LogisticRegression(penalty=None)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "x_test = np.array([[1,11,10,9],[1,3,4,3],[1,1,0,1]])\n",
        "\n",
        "print(model.predict(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtiUVC79xamH",
        "outputId": "9e400337-77e8-4312-ee34-4dc019a8cb54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2]\n"
          ]
        }
      ]
    }
  ]
}